{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing data\n",
    "\n",
    "Load lyrics with artist info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "chunking lyrics\n",
      "{'out_words': ['i', 'smiled', 'at', 'her', 'with', 'a', 'mouth', 'full', 'of', 'gold', 'slugs', '<EOL>', 'yeah', 'nigga', ',', 'we', 'hurtin', 'em', 'bad', '<EOL>', 'droppin', \"'\", 'these', 'bangin', \"'\", 'ass', 'beats', '<EOL>', 'better', 'go', 'warn', 'ya', 'niggaz', 'because', '<EOL>', 'chamillionaire', 'is', 'comin', \"'\", 'with', 'heat', '<END>'], 'inp_ids': tensor([ 1,  6,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  3,  4,\n",
      "         4,  5,  4,  4,  4,  4,  3,  4,  4,  4,  4,  4,  4,  4,\n",
      "         3,  4,  4,  4,  4,  4,  4,  3,  4,  4,  4,  4,  4,  4]), 'artist_id': [], 'out_ids': tensor([ 6,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  3,  4,  4,\n",
      "         5,  4,  4,  4,  4,  3,  4,  4,  4,  4,  4,  4,  4,  3,\n",
      "         4,  4,  4,  4,  4,  4,  3,  4,  4,  4,  4,  4,  4,  2]), 'artist': [], 'inp_words': ['<START>', 'i', 'smiled', 'at', 'her', 'with', 'a', 'mouth', 'full', 'of', 'gold', 'slugs', '<EOL>', 'yeah', 'nigga', ',', 'we', 'hurtin', 'em', 'bad', '<EOL>', 'droppin', \"'\", 'these', 'bangin', \"'\", 'ass', 'beats', '<EOL>', 'better', 'go', 'warn', 'ya', 'niggaz', 'because', '<EOL>', 'chamillionaire', 'is', 'comin', \"'\", 'with', 'heat']} 371574\n"
     ]
    }
   ],
   "source": [
    "import sys,os\n",
    "import string, re\n",
    "import unidecode\n",
    "import random, math, time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import tensorboardX\n",
    "import matplotlib as plt\n",
    "from collections import Counter\n",
    "from torch import nn\n",
    "from torch.nn.utils import rnn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "MAX_LEN = 50\n",
    "BATCH_SIZE = 8\n",
    "USE_ARTIST = False\n",
    "\n",
    "# special tokens\n",
    "EOL = '<EOL>'\n",
    "UNK = '<UNK>'\n",
    "START = '<START>'\n",
    "END = '<END>'\n",
    "PAD = '<padding>'\n",
    "PAD_ID = 0\n",
    "\n",
    "    \n",
    "class LyricsDataset(Dataset):\n",
    "    def __init__(self, pkl_file, vocab_file=None, vocab_size=10000, chunk_size=0, use_artist=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with lyrics.\n",
    "            chunk_size (int): Number of lyric lines to use as single sample. If 0, use song's entire lyrics\n",
    "        \"\"\"\n",
    "        self.lyrics = pickle.load(open(pkl_file,'rb'), encoding='latin1')\n",
    "        \n",
    "        self.vocab_len = vocab_size\n",
    "        if vocab_file == None:\n",
    "            vocab_file = re.sub('.pkl','.vocab',pkl_file)\n",
    "            if not os.path.exists(vocab_file):\n",
    "                self.create_vocab(vocab_file)\n",
    "            \n",
    "        self.vocab = [x.split()[0] for x in open(vocab_file).read().splitlines()][:self.vocab_len]\n",
    "        self.vocab = [START, END, EOL, UNK] + self.vocab\n",
    "        self.vocab.insert(PAD_ID, PAD)\n",
    "        \n",
    "        self.use_artist = use_artist\n",
    "        if self.use_artist:\n",
    "            self.artists = sorted(set([x['artist'] for x in self.lyrics]))\n",
    "            self.num_artists = len(self.artists)\n",
    "            \n",
    "        # chunk lyrics\n",
    "        print(\"chunking lyrics\")\n",
    "        self.chunk_size = chunk_size\n",
    "        if self.chunk_size > 0:\n",
    "            chunked_lyrics = []\n",
    "            for song in self.lyrics:\n",
    "                lines = re.split(r'\\n',song['lyrics'])\n",
    "                for i in range(len(lines) - self.chunk_size+1):\n",
    "                    chunk = '\\n'.join(lines[i:i+self.chunk_size])\n",
    "                    song['lyrics'] = chunk\n",
    "                    chunked_lyrics += [song.copy()]\n",
    "            self.lyrics = chunked_lyrics\n",
    "                    \n",
    "    def create_vocab(self,file_name):\n",
    "        num_songs = len(self.lyrics)\n",
    "        print('creating vocabulary for %d songs'%num_songs)\n",
    "        \n",
    "        vocab = []\n",
    "        for i,e in enumerate(self.lyrics):\n",
    "            if i%(num_songs/10)==0:\n",
    "                print('finished %d/%d songs (%.2f%%)'%(i,num_songs,float(i)/num_songs))\n",
    "            vocab += [w.lower() for w in e['lyrics'].split()]\n",
    "        vocab = Counter(vocab)\n",
    "        \n",
    "        # save up to 100,000 words\n",
    "        with open(file_name,'w') as f:\n",
    "            for i,(a,n) in enumerate(vocab.most_common()):\n",
    "                if i==100000:\n",
    "                    break\n",
    "                if n < 5:\n",
    "                    break\n",
    "                f.write('%s\\t%s\\n'%(a,n))\n",
    "\n",
    "    def __len__(self):\n",
    "        # or length of chunked lyrics?\n",
    "        return len(self.lyrics)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        samp = self.lyrics[idx]\n",
    "        sample = {'inp_words':[],'out_words':[],'inp_ids':[],'out_ids':[],'artist':[],'artist_id':[]}\n",
    "        tokenized_lyrics = [START] + re.sub('\\n',' %s '%EOL, samp['lyrics']).split() + [END]\n",
    "        \n",
    "        sample['inp_words'] = tokenized_lyrics[:-1][:MAX_LEN]\n",
    "        sample['out_words'] = tokenized_lyrics[1:MAX_LEN+1]\n",
    "        sample['inp_ids'] = self.word_tensor(sample['inp_words'])\n",
    "        sample['out_ids'] = self.word_tensor(sample['out_words'])\n",
    "        \n",
    "        if self.use_artist:\n",
    "            sample['artist'] = samp['artist']\n",
    "            sample['artist_id'] = self.artists.index(sample['artist'])\n",
    "    \n",
    "        return sample\n",
    "        \n",
    "    # Turn list of words into list of longs\n",
    "    def word_tensor(self,words):\n",
    "        tensor = torch.zeros(len(words)).long()\n",
    "        for w in range(len(words)):\n",
    "            try:\n",
    "                tensor[w] = self.vocab.index(words[w])\n",
    "            except Exception as e:\n",
    "                tensor[w] = self.vocab.index(UNK)\n",
    "        return Variable(tensor)\n",
    "\n",
    "    def word2id(word):\n",
    "        try:\n",
    "            idx = self.vocab.index(word)\n",
    "        except Exception as e:\n",
    "            idx = self.vocab.index(UNK)\n",
    "        return idx\n",
    "    \n",
    "    def id2word(idx):\n",
    "        return self.vocab[idx]\n",
    "\n",
    "Data = LyricsDataset('lyrics/input_files/artists_train.pkl', vocab_file='lyrics/input_files/lyrics_top_artists.vocab', \n",
    "                     vocab_size=5,chunk_size=5,use_artist=USE_ARTIST)\n",
    "print(Data[np.random.randint(len(Data))], len(Data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[ 1,  4,  8,  4,  4,  4,  4,  4,  4,  4,  4,  5,  3,  4,\n",
      "          5,  9,  4,  4,  4,  4,  8,  4,  5,  3,  4,  4,  4,  4,\n",
      "          8,  4,  4,  4,  4,  5,  3,  9,  6,  5,  4,  7,  4,  4,\n",
      "          8,  4,  4,  6,  4,  5,  3,  4],\n",
      "        [ 1,  6,  4,  4,  4,  8,  4,  5,  4,  4,  4,  4,  3,  6,\n",
      "          4,  4,  4,  8,  4,  9,  4,  4,  4,  3,  9,  4,  8,  4,\n",
      "          4,  4,  4,  4,  8,  4,  3,  4,  4,  5,  4,  4,  5,  4,\n",
      "          4,  4,  4,  4,  4,  3,  4,  4],\n",
      "        [ 1,  4,  7,  4,  9,  4,  4,  4,  3,  4,  4,  4,  5,  7,\n",
      "          4,  4,  3,  4,  5,  4,  5,  4,  5,  7,  4,  4,  3,  4,\n",
      "          6,  4,  4,  4,  6,  4,  3,  9,  4,  4,  4,  4,  4,  4,\n",
      "          4,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 1,  4,  6,  4,  4,  4,  3,  4,  4,  4,  4,  4,  4,  4,\n",
      "          3,  4,  4,  4,  4,  4,  4,  4,  3,  4,  4,  4,  4,  9,\n",
      "          4,  3,  7,  4,  4,  4,  4,  7,  4,  4,  4,  4,  4,  4,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 1,  7,  4,  4,  4,  4,  4,  4,  3,  7,  4,  4,  4,  4,\n",
      "          4,  4,  3,  6,  4,  4,  4,  4,  4,  4,  4,  3,  4,  4,\n",
      "          4,  4,  4,  4,  3,  4,  4,  4,  4,  4,  4,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 1,  4,  4,  4,  4,  4,  3,  4,  4,  4,  3,  4,  4,  4,\n",
      "          4,  4,  3,  4,  4,  4,  4,  4,  3,  9,  6,  4,  4,  4,\n",
      "          4,  4,  4,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 1,  4,  4,  4,  3,  4,  4,  4,  3,  4,  4,  3,  4,  4,\n",
      "          4,  4,  4,  3,  4,  4,  4,  4,  4,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 1,  4,  4,  4,  4,  3,  4,  4,  4,  4,  3,  4,  4,  4,\n",
      "          3,  4,  4,  4,  3,  4,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0]]), [50, 50, 43, 42, 39, 31, 23, 20], tensor([[ 4,  8,  4,  4,  4,  4,  4,  4,  4,  4,  5,  3,  4,  5,\n",
      "          9,  4,  4,  4,  4,  8,  4,  5,  3,  4,  4,  4,  4,  8,\n",
      "          4,  4,  4,  4,  5,  3,  9,  6,  5,  4,  7,  4,  4,  8,\n",
      "          4,  4,  6,  4,  5,  3,  4,  4],\n",
      "        [ 6,  4,  4,  4,  8,  4,  5,  4,  4,  4,  4,  3,  6,  4,\n",
      "          4,  4,  8,  4,  9,  4,  4,  4,  3,  9,  4,  8,  4,  4,\n",
      "          4,  4,  4,  8,  4,  3,  4,  4,  5,  4,  4,  5,  4,  4,\n",
      "          4,  4,  4,  4,  3,  4,  4,  5],\n",
      "        [ 4,  7,  4,  9,  4,  4,  4,  3,  4,  4,  4,  5,  7,  4,\n",
      "          4,  3,  4,  5,  4,  5,  4,  5,  7,  4,  4,  3,  4,  6,\n",
      "          4,  4,  4,  6,  4,  3,  9,  4,  4,  4,  4,  4,  4,  4,\n",
      "          2,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 4,  6,  4,  4,  4,  3,  4,  4,  4,  4,  4,  4,  4,  3,\n",
      "          4,  4,  4,  4,  4,  4,  4,  3,  4,  4,  4,  4,  9,  4,\n",
      "          3,  7,  4,  4,  4,  4,  7,  4,  4,  4,  4,  4,  4,  2,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 7,  4,  4,  4,  4,  4,  4,  3,  7,  4,  4,  4,  4,  4,\n",
      "          4,  3,  6,  4,  4,  4,  4,  4,  4,  4,  3,  4,  4,  4,\n",
      "          4,  4,  4,  3,  4,  4,  4,  4,  4,  4,  2,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 4,  4,  4,  4,  4,  3,  4,  4,  4,  3,  4,  4,  4,  4,\n",
      "          4,  3,  4,  4,  4,  4,  4,  3,  9,  6,  4,  4,  4,  4,\n",
      "          4,  4,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 4,  4,  4,  3,  4,  4,  4,  3,  4,  4,  3,  4,  4,  4,\n",
      "          4,  4,  3,  4,  4,  4,  4,  4,  2,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 4,  4,  4,  4,  3,  4,  4,  4,  4,  3,  4,  4,  4,  3,\n",
      "          4,  4,  4,  3,  4,  2,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0]]), [50, 50, 43, 42, 39, 31, 23, 20], None, [{'out_words': ['all', 'the', 'pieces', 'in', 'my', 'mind', 'make', 'a', 'perfect', 'picture', ',', '<EOL>', 'yeah', ',', 'and', 'every', 'time', 'we', 'on', 'the', 'phone', ',', '<EOL>', 'sitting', 'down', 'reminiscing', 'bout', 'the', 'times', 'we', 'are', 'alone', ',', '<EOL>', 'and', 'i', ',', 'tell', 'you', 'stores', 'bout', 'the', 'stupid', 'things', 'i', 'do', ',', '<EOL>', 'listening', 'to'], 'inp_ids': tensor([ 1,  4,  8,  4,  4,  4,  4,  4,  4,  4,  4,  5,  3,  4,\n",
      "         5,  9,  4,  4,  4,  4,  8,  4,  5,  3,  4,  4,  4,  4,\n",
      "         8,  4,  4,  4,  4,  5,  3,  9,  6,  5,  4,  7,  4,  4,\n",
      "         8,  4,  4,  6,  4,  5,  3,  4]), 'artist_id': [], 'out_ids': tensor([ 4,  8,  4,  4,  4,  4,  4,  4,  4,  4,  5,  3,  4,  5,\n",
      "         9,  4,  4,  4,  4,  8,  4,  5,  3,  4,  4,  4,  4,  8,\n",
      "         4,  4,  4,  4,  5,  3,  9,  6,  5,  4,  7,  4,  4,  8,\n",
      "         4,  4,  6,  4,  5,  3,  4,  4]), 'artist': [], 'inp_words': ['<START>', 'all', 'the', 'pieces', 'in', 'my', 'mind', 'make', 'a', 'perfect', 'picture', ',', '<EOL>', 'yeah', ',', 'and', 'every', 'time', 'we', 'on', 'the', 'phone', ',', '<EOL>', 'sitting', 'down', 'reminiscing', 'bout', 'the', 'times', 'we', 'are', 'alone', ',', '<EOL>', 'and', 'i', ',', 'tell', 'you', 'stores', 'bout', 'the', 'stupid', 'things', 'i', 'do', ',', '<EOL>', 'listening']}, {'out_words': ['i', 'wake', 'up', 'in', 'the', 'morning', ',', 'thinking', \"'bout\", 'my', 'troubles', '<EOL>', 'i', 'go', 'down', 'to', 'the', 'water', 'and', 'they', 'pass', 'away', '<EOL>', 'and', 'when', 'the', 'old', 'man', 'comes', 'a-floating', 'down', 'the', 'river', '<EOL>', '``', 'hey', ',', 'old', 'man', ',', 'are', 'they', 'bitin', \"'\", 'today', \"''\", '<EOL>', '``', 'hey', ','], 'inp_ids': tensor([ 1,  6,  4,  4,  4,  8,  4,  5,  4,  4,  4,  4,  3,  6,\n",
      "         4,  4,  4,  8,  4,  9,  4,  4,  4,  3,  9,  4,  8,  4,\n",
      "         4,  4,  4,  4,  8,  4,  3,  4,  4,  5,  4,  4,  5,  4,\n",
      "         4,  4,  4,  4,  4,  3,  4,  4]), 'artist_id': [], 'out_ids': tensor([ 6,  4,  4,  4,  8,  4,  5,  4,  4,  4,  4,  3,  6,  4,\n",
      "         4,  4,  8,  4,  9,  4,  4,  4,  3,  9,  4,  8,  4,  4,\n",
      "         4,  4,  4,  8,  4,  3,  4,  4,  5,  4,  4,  5,  4,  4,\n",
      "         4,  4,  4,  4,  3,  4,  4,  5]), 'artist': [], 'inp_words': ['<START>', 'i', 'wake', 'up', 'in', 'the', 'morning', ',', 'thinking', \"'bout\", 'my', 'troubles', '<EOL>', 'i', 'go', 'down', 'to', 'the', 'water', 'and', 'they', 'pass', 'away', '<EOL>', 'and', 'when', 'the', 'old', 'man', 'comes', 'a-floating', 'down', 'the', 'river', '<EOL>', '``', 'hey', ',', 'old', 'man', ',', 'are', 'they', 'bitin', \"'\", 'today', \"''\", '<EOL>', '``', 'hey']}, {'out_words': ['if', 'you', 'lie', 'and', 'cheat', 'so', 'easily', '<EOL>', 'ya', 'better', 'go', ',', 'you', 'better', 'go', '<EOL>', 'go', ',', 'go', ',', 'go', ',', 'you', 'better', 'go', '<EOL>', 'all', 'i', 'know', 'is', 'how', 'i', 'feel', '<EOL>', 'and', 'my', 'heart', 'was', 'always', 'yours', 'to', 'steal', '<END>'], 'inp_ids': tensor([ 1,  4,  7,  4,  9,  4,  4,  4,  3,  4,  4,  4,  5,  7,\n",
      "         4,  4,  3,  4,  5,  4,  5,  4,  5,  7,  4,  4,  3,  4,\n",
      "         6,  4,  4,  4,  6,  4,  3,  9,  4,  4,  4,  4,  4,  4,\n",
      "         4]), 'artist_id': [], 'out_ids': tensor([ 4,  7,  4,  9,  4,  4,  4,  3,  4,  4,  4,  5,  7,  4,\n",
      "         4,  3,  4,  5,  4,  5,  4,  5,  7,  4,  4,  3,  4,  6,\n",
      "         4,  4,  4,  6,  4,  3,  9,  4,  4,  4,  4,  4,  4,  4,\n",
      "         2]), 'artist': [], 'inp_words': ['<START>', 'if', 'you', 'lie', 'and', 'cheat', 'so', 'easily', '<EOL>', 'ya', 'better', 'go', ',', 'you', 'better', 'go', '<EOL>', 'go', ',', 'go', ',', 'go', ',', 'you', 'better', 'go', '<EOL>', 'all', 'i', 'know', 'is', 'how', 'i', 'feel', '<EOL>', 'and', 'my', 'heart', 'was', 'always', 'yours', 'to', 'steal']}, {'out_words': ['if', 'i', 'got', 'to', 'kill', '<EOL>', 'if', 'niggas', 'get', 'to', 'fuckin', \"'\", 'around', '<EOL>', 'if', 'niggas', 'get', 'to', 'fuckin', \"'\", 'around', '<EOL>', 'respect', 'come', 'from', 'admiration', 'and', 'fear', '<EOL>', 'you', 'can', 'admire', 'me', 'if', 'you', 'can', 'catch', 'one', 'in', 'your', 'wig', '<END>'], 'inp_ids': tensor([ 1,  4,  6,  4,  4,  4,  3,  4,  4,  4,  4,  4,  4,  4,\n",
      "         3,  4,  4,  4,  4,  4,  4,  4,  3,  4,  4,  4,  4,  9,\n",
      "         4,  3,  7,  4,  4,  4,  4,  7,  4,  4,  4,  4,  4,  4]), 'artist_id': [], 'out_ids': tensor([ 4,  6,  4,  4,  4,  3,  4,  4,  4,  4,  4,  4,  4,  3,\n",
      "         4,  4,  4,  4,  4,  4,  4,  3,  4,  4,  4,  4,  9,  4,\n",
      "         3,  7,  4,  4,  4,  4,  7,  4,  4,  4,  4,  4,  4,  2]), 'artist': [], 'inp_words': ['<START>', 'if', 'i', 'got', 'to', 'kill', '<EOL>', 'if', 'niggas', 'get', 'to', 'fuckin', \"'\", 'around', '<EOL>', 'if', 'niggas', 'get', 'to', 'fuckin', \"'\", 'around', '<EOL>', 'respect', 'come', 'from', 'admiration', 'and', 'fear', '<EOL>', 'you', 'can', 'admire', 'me', 'if', 'you', 'can', 'catch', 'one', 'in', 'your', 'wig']}, {'out_words': ['you', \"'ve\", 'got', 'to', 'lay', 'it', 'down', '<EOL>', 'you', \"'ve\", 'got', 'to', 'lay', 'it', 'down', '<EOL>', 'i', 'said', 'lay', 'down', 'your', 'burden', 'of', 'sorrow', '<EOL>', 'lay', 'down', 'your', 'burden', 'of', 'hurt', '<EOL>', 'lay', 'down', 'your', 'burden', 'of', 'sorrow', '<END>'], 'inp_ids': tensor([ 1,  7,  4,  4,  4,  4,  4,  4,  3,  7,  4,  4,  4,  4,\n",
      "         4,  4,  3,  6,  4,  4,  4,  4,  4,  4,  4,  3,  4,  4,\n",
      "         4,  4,  4,  4,  3,  4,  4,  4,  4,  4,  4]), 'artist_id': [], 'out_ids': tensor([ 7,  4,  4,  4,  4,  4,  4,  3,  7,  4,  4,  4,  4,  4,\n",
      "         4,  3,  6,  4,  4,  4,  4,  4,  4,  4,  3,  4,  4,  4,\n",
      "         4,  4,  4,  3,  4,  4,  4,  4,  4,  4,  2]), 'artist': [], 'inp_words': ['<START>', 'you', \"'ve\", 'got', 'to', 'lay', 'it', 'down', '<EOL>', 'you', \"'ve\", 'got', 'to', 'lay', 'it', 'down', '<EOL>', 'i', 'said', 'lay', 'down', 'your', 'burden', 'of', 'sorrow', '<EOL>', 'lay', 'down', 'your', 'burden', 'of', 'hurt', '<EOL>', 'lay', 'down', 'your', 'burden', 'of', 'sorrow']}, {'out_words': ['some', 'things', 'we', 'could', 'do', '<EOL>', 'in', 'our', 'madness', '<EOL>', 'we', 'burnt', 'one', 'hundred', 'days', '<EOL>', 'time', 'takes', 'time', 'to', 'pass', '<EOL>', 'and', 'i', 'still', 'hold', 'some', 'ashes', 'to', 'me', '<END>'], 'inp_ids': tensor([ 1,  4,  4,  4,  4,  4,  3,  4,  4,  4,  3,  4,  4,  4,\n",
      "         4,  4,  3,  4,  4,  4,  4,  4,  3,  9,  6,  4,  4,  4,\n",
      "         4,  4,  4]), 'artist_id': [], 'out_ids': tensor([ 4,  4,  4,  4,  4,  3,  4,  4,  4,  3,  4,  4,  4,  4,\n",
      "         4,  3,  4,  4,  4,  4,  4,  3,  9,  6,  4,  4,  4,  4,\n",
      "         4,  4,  2]), 'artist': [], 'inp_words': ['<START>', 'some', 'things', 'we', 'could', 'do', '<EOL>', 'in', 'our', 'madness', '<EOL>', 'we', 'burnt', 'one', 'hundred', 'days', '<EOL>', 'time', 'takes', 'time', 'to', 'pass', '<EOL>', 'and', 'i', 'still', 'hold', 'some', 'ashes', 'to', 'me']}, {'out_words': ['ni', 'ta', 'vie', '<EOL>', 'ni', 'tes', 'nuits', '<EOL>', 'ce', 'jeu', '<EOL>', 'do', \"n't\", 'burn', 'your', 'fingers', '<EOL>', 'do', \"n't\", 'burn', 'your', 'fingers', '<END>'], 'inp_ids': tensor([ 1,  4,  4,  4,  3,  4,  4,  4,  3,  4,  4,  3,  4,  4,\n",
      "         4,  4,  4,  3,  4,  4,  4,  4,  4]), 'artist_id': [], 'out_ids': tensor([ 4,  4,  4,  3,  4,  4,  4,  3,  4,  4,  3,  4,  4,  4,\n",
      "         4,  4,  3,  4,  4,  4,  4,  4,  2]), 'artist': [], 'inp_words': ['<START>', 'ni', 'ta', 'vie', '<EOL>', 'ni', 'tes', 'nuits', '<EOL>', 'ce', 'jeu', '<EOL>', 'do', \"n't\", 'burn', 'your', 'fingers', '<EOL>', 'do', \"n't\", 'burn', 'your', 'fingers']}, {'out_words': ['a', 'son', 'portrait', 'fidã¨le', '<EOL>', 'il', \"n'why\", 'a', \"qu'elle\", '<EOL>', 'et', \"see'est\", 'elle', '<EOL>', 'mon', 'mod', '...', '<EOL>', 'elle', '<END>'], 'inp_ids': tensor([ 1,  4,  4,  4,  4,  3,  4,  4,  4,  4,  3,  4,  4,  4,\n",
      "         3,  4,  4,  4,  3,  4]), 'artist_id': [], 'out_ids': tensor([ 4,  4,  4,  4,  3,  4,  4,  4,  4,  3,  4,  4,  4,  3,\n",
      "         4,  4,  4,  3,  4,  2]), 'artist': [], 'inp_words': ['<START>', 'a', 'son', 'portrait', 'fidã¨le', '<EOL>', 'il', \"n'why\", 'a', \"qu'elle\", '<EOL>', 'et', \"see'est\", 'elle', '<EOL>', 'mon', 'mod', '...', '<EOL>', 'elle']}])\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'lyrics/artists_val.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-929216226798>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m ValData = LyricsDataset('lyrics/artists_val.pkl', vocab_file='lyrics/input_files/lyrics_top_artists.vocab', \n\u001b[0;32m---> 34\u001b[0;31m                         chunk_size=5,use_artist=USE_ARTIST)\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0mval_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mValData\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-ce7e68810b1b>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, pkl_file, vocab_file, vocab_size, chunk_size, use_artist)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mchunk_size\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNumber\u001b[0m \u001b[0mof\u001b[0m \u001b[0mlyric\u001b[0m \u001b[0mlines\u001b[0m \u001b[0mto\u001b[0m \u001b[0muse\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msingle\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mIf\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse\u001b[0m \u001b[0msong\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mentire\u001b[0m \u001b[0mlyrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \"\"\"\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlyrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpkl_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'lyrics/artists_val.pkl'"
     ]
    }
   ],
   "source": [
    "def padding(data):\n",
    "    # gets samples (dicts) from Data\n",
    "    \n",
    "    def merge(seqs):\n",
    "        lengths = [len(s) for s in seqs]\n",
    "        max_len = np.max(lengths)\n",
    "        \n",
    "        padded_seqs = torch.ones(len(seqs), max_len).long()*PAD_ID\n",
    "        for i,s in enumerate(seqs):\n",
    "            end = lengths[i]\n",
    "            padded_seqs[i, :end] = s[:end]\n",
    "                \n",
    "        return padded_seqs, lengths\n",
    "    \n",
    "    data.sort(key=lambda x:len(x['inp_ids']),reverse=True)\n",
    "    \n",
    "    inp_seqs,inp_lens = merge([x['inp_ids'] for x in data])\n",
    "    out_seqs,out_lens = merge([x['out_ids'] for x in data])\n",
    "    if Data.use_artist:\n",
    "        inp_artists = torch.from_numpy(np.array([x['artist_id'] for x in data]))\n",
    "    else:\n",
    "        inp_artists = None\n",
    "        \n",
    "    return inp_seqs,inp_lens,out_seqs,out_lens,inp_artists,data\n",
    "\n",
    "\n",
    "dataloader = DataLoader(Data, batch_size=BATCH_SIZE, shuffle=True, num_workers=1, collate_fn=padding)\n",
    "\n",
    "for i,batch in enumerate(dataloader):\n",
    "    print(batch)\n",
    "    break\n",
    "    \n",
    "ValData = LyricsDataset('lyrics/artists_val.pkl', vocab_file='lyrics/input_files/lyrics_top_artists.vocab', \n",
    "                        chunk_size=5,use_artist=USE_ARTIST)\n",
    "val_dataloader = DataLoader(ValData,  batch_size=BATCH_SIZE, num_workers=1, collate_fn=padding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "class LyricsRNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, batch_size=BATCH_SIZE, \n",
    "                 n_layers=1, hidden_size=256, word_embedding_size=128, \n",
    "                 use_artist=True, embed_artist=False, num_artists=10, artist_embedding_size=32):\n",
    "        \n",
    "        super(LyricsRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.batchsize = batch_size\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.word_embed_size = word_embedding_size\n",
    "        self.word_encoder = nn.Embedding(self.input_size, self.word_embed_size,padding_idx=PAD_ID)\n",
    "        self.lstm_input_size = self.word_embed_size\n",
    "        \n",
    "        self.use_artist = use_artist\n",
    "        if self.use_artist:\n",
    "            self.num_artists = num_artists\n",
    "            # either embed artist data or use a one-hot vector\n",
    "            if embed_artist:\n",
    "                self.artist_embed_size = artist_embedding_size\n",
    "                self.artist_encoder = nn.Embedding(self.num_artists, self.artist_embed_size)\n",
    "            else:\n",
    "                self.artist_embed_size = self.num_artists\n",
    "                self.artist_encoder = self.artist_onehot\n",
    "\n",
    "                    \n",
    "            self.lstm_input_size += self.artist_embed_size\n",
    "            \n",
    "        self.lstm = nn.LSTM(self.lstm_input_size, hidden_size, n_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        self.hidden = self.init_hidden()\n",
    "    \n",
    "    def artist_onehot(self, artist):\n",
    "        tensor = torch.zeros(self.batchsize,artist.size()[1],self.num_artists).to(device)\n",
    "        for i in range(tensor.size()[0]):\n",
    "            idx = artist[i,0]\n",
    "            tensor[i,:,idx] = 1\n",
    "        return tensor\n",
    "    \n",
    "    def init_hidden(self):\n",
    "         (Variable(torch.randn(self.n_layers, self.batchsize, self.hidden_size)).to(device),\n",
    "                Variable(torch.randn(self.n_layers, self.batchsize, self.hidden_size)).to(device))\n",
    "\n",
    "    def forward(self, input, input_lens):\n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "        if self.use_artist:\n",
    "            input,artist_input = input\n",
    "        \n",
    "        embed = self.word_encoder(input)\n",
    "        \n",
    "        if self.use_artist:\n",
    "            # repeat artist along sequence\n",
    "            artist_input = torch.unsqueeze(artist_input,dim=1)\n",
    "            artist_input = artist_input.expand(-1,input.size()[1]).to(device)\n",
    "            \n",
    "            artist_embed = self.artist_encoder(artist_input)\n",
    "            \n",
    "            # concatenate artist embedding to word embeddings\n",
    "            embed = torch.cat([embed,artist_embed],dim=2)\n",
    "                    \n",
    "        emb_pad = rnn.pack_padded_sequence(embed, input_lens, batch_first=True)\n",
    "        out_pad, self.hidden = self.lstm(emb_pad, self.hidden)\n",
    "        output, _ = rnn.pad_packed_sequence(out_pad, batch_first=True)\n",
    "        \n",
    "        # second RNN goes here\n",
    "\n",
    "        output = output.contiguous().view(-1,output.shape[2])\n",
    "        output = self.linear(output)\n",
    "        output = F.log_softmax(output,dim=1)\n",
    "        output = output.view(self.batchsize, -1, self.output_size)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def loss(self, Y_hat, Y):\n",
    "        print(Y)\n",
    "        print(Y_hat)\n",
    "        Y = Y.view(-1)\n",
    "        Y_hat = Y_hat.view(-1,self.output_size)\n",
    "        mask = (Y != PAD_ID).float()\n",
    "        \n",
    "        non_pad_tokens = torch.sum(mask).item()\n",
    "\n",
    "        Y_hat = Y_hat[range(Y_hat.shape[0]), Y] * mask\n",
    "        \n",
    "        loss = -torch.sum(Y_hat) / non_pad_tokens\n",
    "        return loss\n",
    "    \n",
    "    def evaluate(self, prime_str=[START], artist=None, predict_len=100, temperature=0.8):\n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "        # repeat input across batches\n",
    "        prime_input = Data.word_tensor(prime_str).expand(self.batchsize,-1).to(device)\n",
    "        predicted = prime_str\n",
    "        input_lens = [len(prime_str)-1]*self.batchsize\n",
    "        if self.use_artist:\n",
    "            artist = torch.from_numpy(np.array([artist]*self.batchsize))\n",
    "            \n",
    "        def get_input(inp):\n",
    "            if self.use_artist:\n",
    "                return [inp, artist]\n",
    "            else:\n",
    "                return inp\n",
    "\n",
    "        if len(prime_str) > 1:\n",
    "            # Use priming string to \"build up\" hidden state\n",
    "            self.forward(get_input(prime_input[:,:-1]), input_lens)\n",
    "            \n",
    "        inp = prime_input[:,-1].view(self.batchsize,1).to(device)\n",
    "        input_lens = [1]*self.batchsize\n",
    "        \n",
    "        for p in range(predict_len):\n",
    "            # just get first row, since all rows are the same\n",
    "            output = self.forward(get_input(inp), input_lens)[0]\n",
    "\n",
    "            # Sample from the network as a multinomial distribution\n",
    "            output_dist = output.data.view(-1).div(temperature).exp()\n",
    "            top_i = torch.multinomial(output_dist, 1)[0]\n",
    "\n",
    "            # Add predicted character to string and use as next input\n",
    "            predicted_word = Data.vocab[top_i]\n",
    "            predicted += [predicted_word]\n",
    "            \n",
    "            if predicted_word == END:\n",
    "                break\n",
    "            print(top_i)\n",
    "            print(Data.word_tensor([predicted_word]))\n",
    "                \n",
    "            inp = Data.word_tensor([predicted_word]).expand(self.batchsize,1).to(device)\n",
    "\n",
    "        return ' '.join(predicted)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (59) : device-side assert triggered at /pytorch/aten/src/THC/generic/THCTensorCopy.c:20",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-eb95b2731bd7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#                  n_layers=1, hidden_size=256, word_embedding_size=128,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#                  use_artist=True, embed_artist=False, num_artists=10, artist_embedding_size=32)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLyricsRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_artist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# , num_artists=Data.num_artists,hidden_size=6, word_embedding_size=10,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-e8e80af1954c>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_size, output_size, batch_size, n_layers, hidden_size, word_embedding_size, use_artist, embed_artist, num_artists, artist_embedding_size)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0martist_onehot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-e8e80af1954c>\u001b[0m in \u001b[0;36minit_hidden\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minit_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m          (Variable(torch.randn(self.n_layers, self.batchsize, self.hidden_size)).to(device),\n\u001b[0m\u001b[1;32m     46\u001b[0m                 Variable(torch.randn(self.n_layers, self.batchsize, self.hidden_size)).to(device))\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (59) : device-side assert triggered at /pytorch/aten/src/THC/generic/THCTensorCopy.c:20"
     ]
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "print_every = 1000\n",
    "plot_every = 1000\n",
    "lr = 0.005\n",
    "\n",
    "#     def __init__(self, input_size, output_size, batch_size=BATCH_SIZE, \n",
    "#                  n_layers=1, hidden_size=256, word_embedding_size=128, \n",
    "#                  use_artist=True, embed_artist=False, num_artists=10, artist_embedding_size=32)\n",
    "model = LyricsRNN(Data.vocab_len, Data.vocab_len, use_artist=False,batch_size=3).to(device) # , num_artists=Data.num_artists,hidden_size=6, word_embedding_size=10, \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "start = time.time()\n",
    "all_losses = []\n",
    "loss_avg = 0\n",
    "\n",
    "def time_since(since):\n",
    "    s = time.time() - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        inp_seqs,inp_lens,out_seqs,out_lens,inp_artists,data = batch\n",
    "        \n",
    "        if Data.use_artist:\n",
    "            inp, target = [inp_seqs.to(device),inp_artists.to_device()], out_seqs.to(device)\n",
    "        else:\n",
    "            inp, target = inp_seqs.to(device), out_seqs.to(device)\n",
    "        model.zero_grad()\n",
    "        \n",
    "        predictions = model(inp, inp_lens)\n",
    "        model.evaluate()\n",
    "        break\n",
    "        loss = model.loss(predictions, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_avg += loss\n",
    "\n",
    "        if i % print_every == 0:\n",
    "            print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss))\n",
    "            print(model.evaluate(), '\\n')\n",
    "\n",
    "        if i % plot_every == 0:\n",
    "            all_losses.append(loss_avg / plot_every)\n",
    "            loss_avg = 0\n",
    "    break\n",
    "    val_loss = 0\n",
    "    for i,batch in enumerate(val_dataloader):\n",
    "        inp_seqs,inp_lens,out_seqs,out_lens,inp_artists,data = batch\n",
    "        \n",
    "        if Data.use_artist:\n",
    "            inp, target = [inp_seqs.to(device),inp_artists], out_seqs.to(device)\n",
    "        else:\n",
    "            inp, target = inp_seqs.to(device), out_seqs.to(device)\n",
    "        model.zero_grad()\n",
    "        \n",
    "        predictions = model(inp, inp_lens)\n",
    "        loss = model.loss(predictions, target)\n",
    "        val_loss += loss\n",
    "    avg_val_loss = val_loss / i\n",
    "    print('Validation loss: %.4f'%avg_val_loss)\n",
    "    if avg_val_loss > all_losses[-1]:\n",
    "        break\n",
    "    \n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'losses': all_losses\n",
    "#         'hyperparameters': {\n",
    "#             'input_file': pkl_file,\n",
    "#             'vocab_file': vocab_file,\n",
    "#             'vocab_size': vocab_size,\n",
    "#             'chunk_size':,\n",
    "#             'max_seq_len':,\n",
    "#             'use_artist':,\n",
    "#             'input_size':,\n",
    "#             'output_size':,\n",
    "#             'batch_size':,\n",
    "#             'n_layers':,\n",
    "#             'hidden_size':,\n",
    "#             'word_embedding_size':,\n",
    "#             'num_artists':,\n",
    "#             'artist_embedding_size':\n",
    "#         }\n",
    "    }, 'checkpoints/lyrics_model-e%05d.pt'%epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'matplotlib' has no attribute 'figure'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-cee4e9858e79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'matplotlib' has no attribute 'figure'"
     ]
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(all_losses)\n",
    "\n",
    "print(model.evaluate())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
