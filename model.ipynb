{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing data\n",
    "\n",
    "Load lyrics with artist info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "chunking lyrics\n",
      "{'artist': [], 'out_words': ['riding', 'down', 'santa', 'claus', 'lane', '<EOL>', 'he', \"'s\", 'got', 'a', 'bag', 'that', \"'s\", 'filled', 'with', 'toys', '<EOL>', 'for', 'boys', 'and', 'girls', 'again', '<EOL>', 'hear', 'those', 'sleigh', 'bells', 'jingle', 'jangle', '<EOL>', 'what', 'a', 'beautiful', 'sight', '<END>'], 'inp_ids': tensor([    1,  1089,    61,   710,  1151,  1382,     3,    58,    14,\n",
      "           36,    11,  1112,    18,    14,   882,    35,  1995,     3,\n",
      "           31,   405,     9,   364,   152,     3,   173,   291,  1795,\n",
      "          839,  1944,  7287,     3,    43,    11,   519,   906]), 'out_ids': tensor([ 1089,    61,   710,  1151,  1382,     3,    58,    14,    36,\n",
      "           11,  1112,    18,    14,   882,    35,  1995,     3,    31,\n",
      "          405,     9,   364,   152,     3,   173,   291,  1795,   839,\n",
      "         1944,  7287,     3,    43,    11,   519,   906,     2]), 'inp_words': ['<START>', 'riding', 'down', 'santa', 'claus', 'lane', '<EOL>', 'he', \"'s\", 'got', 'a', 'bag', 'that', \"'s\", 'filled', 'with', 'toys', '<EOL>', 'for', 'boys', 'and', 'girls', 'again', '<EOL>', 'hear', 'those', 'sleigh', 'bells', 'jingle', 'jangle', '<EOL>', 'what', 'a', 'beautiful', 'sight'], 'artist_id': []} 371574\n"
     ]
    }
   ],
   "source": [
    "import sys,os\n",
    "import string, re\n",
    "import unidecode\n",
    "import random, math, time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import tensorboardX\n",
    "import matplotlib as plt\n",
    "from collections import Counter\n",
    "from torch import nn\n",
    "from torch.nn.utils import rnn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "MAX_LEN = 50\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# special tokens\n",
    "EOL = '<EOL>'\n",
    "UNK = '<UNK>'\n",
    "START = '<START>'\n",
    "END = '<END>'\n",
    "PAD = '<padding>'\n",
    "PAD_ID = 0\n",
    "\n",
    "    \n",
    "class LyricsDataset(Dataset):\n",
    "    def __init__(self, pkl_file, vocab_file=None, vocab_size=10000, chunk_size=0, use_artist=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with lyrics.\n",
    "            chunk_size (int): Number of lyric lines to use as single sample. If 0, use song's entire lyrics\n",
    "        \"\"\"\n",
    "        self.lyrics = pickle.load(open(pkl_file,'rb'), encoding='latin1')\n",
    "        \n",
    "        self.vocab_len = vocab_size\n",
    "        if vocab_file == None:\n",
    "            vocab_file = re.sub('.pkl','.vocab',pkl_file)\n",
    "            if not os.path.exists(vocab_file):\n",
    "                self.create_vocab(vocab_file)\n",
    "            \n",
    "        self.vocab = [x.split()[0] for x in open(vocab_file).read().splitlines()][:self.vocab_len]\n",
    "        self.vocab = [START, END, EOL, UNK] + self.vocab\n",
    "        self.vocab.insert(PAD_ID, PAD)\n",
    "        \n",
    "        self.use_artist = use_artist\n",
    "        if self.use_artist:\n",
    "            self.artists = sorted(set([x['artist'] for x in self.lyrics]))\n",
    "            self.num_artists = len(self.artists)\n",
    "            \n",
    "        # chunk lyrics\n",
    "        print(\"chunking lyrics\")\n",
    "        self.chunk_size = chunk_size\n",
    "        if self.chunk_size > 0:\n",
    "            chunked_lyrics = []\n",
    "            for song in self.lyrics:\n",
    "                lines = re.split(r'\\n',song['lyrics'])\n",
    "                for i in range(len(lines) - self.chunk_size+1):\n",
    "                    chunk = '\\n'.join(lines[i:i+self.chunk_size])\n",
    "                    song['lyrics'] = chunk\n",
    "                    chunked_lyrics += [song.copy()]\n",
    "            self.lyrics = chunked_lyrics\n",
    "                    \n",
    "    def create_vocab(self,file_name):\n",
    "        num_songs = len(self.lyrics)\n",
    "        print('creating vocabulary for %d songs'%num_songs)\n",
    "        \n",
    "        vocab = []\n",
    "        for i,e in enumerate(self.lyrics):\n",
    "            if i%(num_songs/10)==0:\n",
    "                print('finished %d/%d songs (%.2f%%)'%(i,num_songs,float(i)/num_songs))\n",
    "            vocab += [w.lower() for w in e['lyrics'].split()]\n",
    "        vocab = Counter(vocab)\n",
    "        \n",
    "        # save up to 100,000 words\n",
    "        with open(file_name,'w') as f:\n",
    "            for i,(a,n) in enumerate(vocab.most_common()):\n",
    "                if i==100000:\n",
    "                    break\n",
    "                if n < 5:\n",
    "                    break\n",
    "                f.write('%s\\t%s\\n'%(a,n))\n",
    "\n",
    "    def __len__(self):\n",
    "        # or length of chunked lyrics?\n",
    "        return len(self.lyrics)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        samp = self.lyrics[idx]\n",
    "        sample = {'inp_words':[],'out_words':[],'inp_ids':[],'out_ids':[],'artist':[],'artist_id':[]}\n",
    "        tokenized_lyrics = [START] + re.sub('\\n',' %s '%EOL, samp['lyrics']).split() + [END]\n",
    "        \n",
    "        sample['inp_words'] = tokenized_lyrics[:-1][:MAX_LEN]\n",
    "        sample['out_words'] = tokenized_lyrics[1:MAX_LEN+1]\n",
    "        sample['inp_ids'] = self.word_tensor(sample['inp_words'])\n",
    "        sample['out_ids'] = self.word_tensor(sample['out_words'])\n",
    "        \n",
    "        if self.use_artist:\n",
    "            sample['artist'] = samp['artist']\n",
    "            sample['artist_id'] = self.artist_tensor(sample['artist'])\n",
    "        \n",
    "        return sample\n",
    "        \n",
    "    # Turn list of words into list of longs\n",
    "    def word_tensor(self,words):\n",
    "        tensor = torch.zeros(len(words)).long()\n",
    "        for w in range(len(words)):\n",
    "            try:\n",
    "                tensor[w] = self.vocab.index(words[w])\n",
    "            except Exception as e:\n",
    "                tensor[w] = self.vocab.index(UNK)\n",
    "        return Variable(tensor)\n",
    "\n",
    "    # returns one hot vector of artists\n",
    "    def artist_tensor(self,artist):\n",
    "        tensor = torch.zeros(self.num_artists).long()\n",
    "        tensor[self.artists.index(artist)] = 1\n",
    "        return tensor\n",
    "    \n",
    "    def word2id(word):\n",
    "        try:\n",
    "            idx = self.vocab.index(word)\n",
    "        except Exception as e:\n",
    "            idx = self.vocab.index(UNK)\n",
    "        return idx\n",
    "    \n",
    "    def id2word(idx):\n",
    "        return self.vocab[idx]\n",
    "\n",
    "Data = LyricsDataset('lyrics/artists_train.pkl', vocab_file='lyrics/lyrics_top_artists.vocab', \n",
    "                     chunk_size=5,use_artist=False)\n",
    "print(Data[np.random.randint(len(Data))], len(Data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunking lyrics\n"
     ]
    }
   ],
   "source": [
    "def padding(data):\n",
    "    # gets samples (dicts) from Data\n",
    "    \n",
    "    def merge(seqs):\n",
    "        lengths = [len(s) for s in seqs]\n",
    "        max_len = np.max(lengths)\n",
    "        \n",
    "        padded_seqs = torch.ones(len(seqs), max_len).long()*PAD_ID\n",
    "        for i,s in enumerate(seqs):\n",
    "            end = lengths[i]\n",
    "            padded_seqs[i, :end] = s[:end]\n",
    "                \n",
    "        return padded_seqs, lengths\n",
    "    \n",
    "    data.sort(key=lambda x:len(x['inp_ids']),reverse=True)\n",
    "    \n",
    "    inp_seqs,inp_lens = merge([x['inp_ids'] for x in data])\n",
    "    out_seqs,out_lens = merge([x['out_ids'] for x in data])\n",
    "    if Data.use_artist:\n",
    "        inp_artists = torch.stack([x['artist_id'] for x in data])\n",
    "    else:\n",
    "        inp_artists = None\n",
    "        \n",
    "    return inp_seqs,inp_lens,out_seqs,out_lens,inp_artists,data\n",
    "\n",
    "\n",
    "dataloader = DataLoader(Data, batch_size=BATCH_SIZE, shuffle=True, num_workers=1, collate_fn=padding)\n",
    "\n",
    "# for i,batch in enumerate(dataloader):\n",
    "#     print(batch)\n",
    "#     break\n",
    "    \n",
    "ValData = LyricsDataset('lyrics/artists_val.pkl', vocab_file='lyrics/lyrics_top_artists.vocab', chunk_size=5)#,use_artist=False)\n",
    "val_dataloader = DataLoader(ValData,  batch_size=BATCH_SIZE, num_workers=1, collate_fn=padding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "class LyricsRNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, batch_size=BATCH_SIZE, \n",
    "                 n_layers=1, hidden_size=256, word_embedding_size=128, \n",
    "                 use_artist=True, num_artists=10, artist_embedding_size=32):\n",
    "        \n",
    "        super(LyricsRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.batchsize = batch_size\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.word_embed_size = word_embedding_size\n",
    "        self.word_encoder = nn.Embedding(self.input_size, self.word_embed_size,padding_idx=PAD_ID)\n",
    "        self.lstm_input_size = self.word_embed_size\n",
    "        \n",
    "        self.use_artist = use_artist\n",
    "        if self.use_artist:\n",
    "            self.num_artists = num_artists\n",
    "            self.artist_embed_size = artist_embedding_size\n",
    "            # may or may not want to use embedding for artist data (maybe just leave as one-hot)\n",
    "            self.artist_encoder = nn.Embedding(self.num_artists, self.artist_embed_size)\n",
    "            self.lstm_input_size += self.artist_embed_size\n",
    "            \n",
    "        self.lstm = nn.LSTM(self.lstm_input_size, hidden_size, n_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        self.hidden = self.init_hidden()\n",
    "    \n",
    "    def init_hidden(self):\n",
    "         (Variable(torch.randn(self.n_layers, self.batchsize, self.hidden_size)).to(device),\n",
    "                Variable(torch.randn(self.n_layers, self.batchsize, self.hidden_size)).to(device))\n",
    "\n",
    "    def forward(self, input, input_lens):\n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "        if self.use_artist:\n",
    "            input,artist_input = input\n",
    "        \n",
    "        embed = self.word_encoder(input)\n",
    "#         print('word embed',embed,embed.size())\n",
    "        \n",
    "        if self.use_artist:\n",
    "            # embed artist\n",
    "            artist_embed = self.artist_encoder(artist_input)\n",
    "            print('artist embed',artist_embed,artist_embed.size())\n",
    "            # concatenate artist embedding to word embeddings\n",
    "            embed = torch.cat([embed,artist_embed])\n",
    "            print('cat embed',embed,embed.size())\n",
    "                    \n",
    "        emb_pad = rnn.pack_padded_sequence(embed, input_lens, batch_first=True)\n",
    "        out_pad, self.hidden = self.lstm(emb_pad, self.hidden)\n",
    "        output, _ = rnn.pad_packed_sequence(out_pad, batch_first=True)\n",
    "\n",
    "        output = output.contiguous().view(-1,output.shape[2])\n",
    "        output = self.linear(output)\n",
    "        output = F.log_softmax(output,dim=1)\n",
    "        output = output.view(self.batchsize, -1, self.output_size)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def loss(self, Y_hat, Y):\n",
    "        Y = Y.view(-1)\n",
    "        Y_hat = Y_hat.view(-1,self.output_size)\n",
    "        mask = (Y != PAD_ID).float()\n",
    "        \n",
    "        non_pad_tokens = torch.sum(mask).item()\n",
    "        Y_hat = Y_hat[range(Y_hat.shape[0]), Y] * mask\n",
    "        \n",
    "        loss = -torch.sum(Y_hat) / non_pad_tokens\n",
    "        return loss\n",
    "    \n",
    "    def evaluate(self, prime_str=[START], artist=None, predict_len=100, temperature=0.8):\n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "        # repeat input across batches\n",
    "        prime_input = Data.word_tensor(prime_str).expand(self.batchsize,-1).to(device)\n",
    "        predicted = prime_str\n",
    "        input_lens = [len(prime_str)-1]*self.batchsize\n",
    "        \n",
    "#         if self.use_artist:\n",
    "\n",
    "        if len(prime_str) > 1:\n",
    "            # Use priming string to \"build up\" hidden state\n",
    "            self.forward(prime_input[:,:-1], input_lens)\n",
    "            \n",
    "        inp = prime_input[:,-1].view(self.batchsize,1).to(device)\n",
    "        input_lens = [1]*self.batchsize\n",
    "        \n",
    "        for p in range(predict_len):\n",
    "            # just get first row, since all rows are the same\n",
    "            output = self.forward(inp, input_lens)[0]\n",
    "\n",
    "            # Sample from the network as a multinomial distribution\n",
    "            output_dist = output.data.view(-1).div(temperature).exp()\n",
    "            top_i = torch.multinomial(output_dist, 1)[0]\n",
    "\n",
    "            # Add predicted character to string and use as next input\n",
    "            predicted_word = Data.vocab[top_i]\n",
    "            predicted += [predicted_word]\n",
    "            \n",
    "            if predicted_word == END:\n",
    "                break\n",
    "                \n",
    "            inp = Data.word_tensor([predicted_word]).expand(self.batchsize,1).to(device)\n",
    "\n",
    "        return ' '.join(predicted)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0m 0s (1 0%) 9.2126]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDNN_STATUS_EXECUTION_FAILED",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-9e9800290be7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp_lens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-b6e1b055dd44>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, input_lens)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0memb_pad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mout_pad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb_pad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_packed_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_pad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0mflat_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         )\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_packed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, *fargs, **fkwargs)\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, weight, hx, batch_sizes)\u001b[0m\n\u001b[1;32m    285\u001b[0m             \u001b[0mbatch_first\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbidirectional\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_sizes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mvariable_length\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m             dropout_ts)\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDNN_STATUS_EXECUTION_FAILED"
     ]
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "print_every = 1000\n",
    "plot_every = 1000\n",
    "lr = 0.005\n",
    "\n",
    "model = LyricsRNN(Data.vocab_len, Data.vocab_len, use_artist=False).to(device) # ,hidden_size=6, word_embedding_size=10, \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "start = time.time()\n",
    "all_losses = []\n",
    "loss_avg = 0\n",
    "\n",
    "def time_since(since):\n",
    "    s = time.time() - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        inp_seqs,inp_lens,out_seqs,out_lens,inp_artists,data = batch\n",
    "        \n",
    "        if Data.use_artist:\n",
    "            inp, target = [inp_seqs.to(device),inp_artists.to(device)], out_seqs.to(device)\n",
    "        else:\n",
    "            inp, target = inp_seqs.to(device), out_seqs.to(device)\n",
    "        model.zero_grad()\n",
    "        \n",
    "        predictions = model(inp, inp_lens)\n",
    "        loss = model.loss(predictions, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_avg += loss\n",
    "\n",
    "        if i % print_every == 0:\n",
    "            print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss))\n",
    "#             print(model.evaluate(), '\\n')\n",
    "\n",
    "        if i % plot_every == 0:\n",
    "            all_losses.append(loss_avg / plot_every)\n",
    "            loss_avg = 0\n",
    "    \n",
    "    val_loss = 0\n",
    "    for i,batch in enumerate(val_dataloader):\n",
    "        inp_seqs,inp_lens,out_seqs,out_lens,inp_artists,data = batch\n",
    "        \n",
    "        if Data.use_artist:\n",
    "            inp, target = [inp_seqs.to(device),inp_artists.to(device)], out_seqs.to(device)\n",
    "        else:\n",
    "            inp, target = inp_seqs.to(device), out_seqs.to(device)\n",
    "        model.zero_grad()\n",
    "        \n",
    "        predictions = model(inp, inp_lens)\n",
    "        loss = model.loss(predictions, target)\n",
    "        val_loss += loss\n",
    "    avg_val_loss = val_loss / i\n",
    "    print('Validation loss: %.4f'%avg_val_loss)\n",
    "    if avg_val_loss > all_losses[-1]:\n",
    "        break\n",
    "\n",
    "        ###\n",
    "# def train(inp, target):\n",
    "#     inp, target = inp.to(device), target.to(device)\n",
    "#     model.zero_grad()\n",
    "#     model.hidden = model.init_hidden()\n",
    "#     loss = 0\n",
    "\n",
    "#     inp_len = len(inp)\n",
    "#     for c in range(inp_len):\n",
    "#         output = model(inp[c])\n",
    "#         loss += criterion(output, target[c].unsqueeze(0))\n",
    "\n",
    "#     loss.backward()\n",
    "#     improved_decoder_optimizer.step()\n",
    "\n",
    "#     return loss.data.item() / chunk_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(all_losses)\n",
    "\n",
    "print(model.evaluate())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
